<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Robert Louka" />
<meta name="author" content="Ryan Sawasaki" />
<meta name="author" content="Joshua Noble" />
<meta name="author" content="Praveen Joseph" />


<title>W203: Statistics for Data Science</title>

<script src="lab_3_master_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="lab_3_master_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="lab_3_master_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="lab_3_master_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="lab_3_master_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="lab_3_master_files/navigation-1.1/tabsets.js"></script>
<link href="lab_3_master_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="lab_3_master_files/highlightjs-9.12.0/highlight.js"></script>
<link href="lab_3_master_files/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="lab_3_master_files/pagedtable-1.1/js/pagedtable.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore"><strong>W203: Statistics for Data Science</strong></h1>
<br/>
<h3 class="subtitle"><strong>LAB 3: Reducing Crime</strong></h3>
<br/>
<table style="width:100%">
<tr>
<td><h4>Robert Louka</h4></td>
<td><h4>Ryan Sawasaki</h4></td>
<td><h4>Joshua Noble</h4></td>
<td><h4>Praveen Joseph</h4></td>
</tr>
</table>

</div>

<br/>
<p><h2>1. An Introduction</h2></p>
<p>As crime has seen an increase in the 1980’s, citizens of North Carolina have been looking to local government politicians to address this growing problem. In preparation for the upcoming election, our team of political consultants has been tasked with providing insight to drive policy directed at reducing crime levels. Before pushing a political campaign aimed at crime reduction, we must first identify the key determinants of crime and their significance in order to properly focus resources to target these issues.</p>
<p>Many studies have examined numerous potential determinants of crimes and it remains a complex and evolving issue. Traditionally, criminal activity is often linked to issues of inequality and poverty. In addition, factors revolving around the criminal justice system are often viewed as having a significant impact, both positive and negative, on crime rate. While there is little debate that these variables affect crime, a one size fits all policy on crime does not properly address the unique issues at the state and county levels. This report aims to identify the complex interactions of crime determinants in North Carolina using recently compiled statistics from FBI and government agencies.</p>
<p>While many studies have been conducted on individual crime factors, this report examines multiple factors holistically. The primary research question this report addresses is: Which demographic, economic and deterrent factors significantly affect crime? To answer this question, our team has been provided a dataset of 1987 statistics from select North Carolina counties. The data has been pulled from multiple credible sources including:<br />
* FBI’s Uniform Crime Reports<br />
* FBI’s police agency employee counts<br />
* North Carolina Department of Correction<br />
* North Carolina Employment Security Commission<br />
* Census Data<br />
</p>
<p>Our dependent variable and the key measure we are focused on is crime rate, which is defined as crimes committed per person. Our independent variables have been grouped into categories of deterrent, demographic, economic, and geographical factors. A comprehensive list of the variables and their respective categories are described in our exploratory data analysis.</p>
<p>While this 1987 dataset provides observational variables that impact crime, the dataset does not provide a comprehensive list of all variables. There are a number of factors that our team has identified that could potentially assist in more accurately measuring a causal effect on crime. These factors are discussed in further detail in the omitted variables section of this report. In addition, this dataset only covers a single cross-section of the data from the year 1987. A multi-year panel of data, if provided, could improve the accuracy of our models and address lead-lag effects. Additional data and experimental studies could provide a more accurate casual model, however the statistical results of this report are limited to the information provided in the 1987 dataset. Using these results, our team has prepared recommendations for a political strategy addressing crime in North Carolina.</p>
<p><h2>2. A Model Building Process</h2></p>
<div id="exploratory-data-analaysis" class="section level1">
<h3>Exploratory Data Analaysis</h3>
<p>We started by conducting exploratory data analysis. First, we read the original paper [CORNWELL – TRUMBULL (1994)] to get a better understanding of each variable. We defined the variables in the table below and grouped them into five groups in order to get a better handle on them.</p>
<pre class="r"><code>crime_count &lt;- c(1:25)
data_variables &lt;- c(&quot;county&quot;,&quot;year&quot;,&quot;crmrte&quot;,&quot;prbarr&quot;,&quot;prbconv&quot;,&quot;prbpris&quot;,&quot;avgsen&quot;,&quot;polpc&quot;,&quot;density&quot;,&quot;taxpc&quot;,&quot;west&quot;,&quot;central&quot;,&quot;urban&quot;,&quot;pctmin80&quot;,&quot;wcon&quot;,&quot;wtuc&quot;,&quot;wtrd&quot;,&quot;wfir&quot;,&quot;wser&quot;,&quot;wmfg&quot;,&quot;wfed&quot;,&quot;wsta&quot;,&quot;wloc&quot;,&quot;mix&quot;,&quot;pctymle&quot;)
data_description &lt;- c(&quot;county identifier&quot;,&quot;1987&quot;,&quot;crimes committed per person&quot;,&quot;&#39;probability&#39; of arrest&quot;,&quot;&#39;probability&#39; of conviction&quot;,&quot;&#39;probability&#39; of prison sentence&quot;,&quot;avg. sentence, days&quot;,&quot;police per capita&quot;,&quot;people per sq. mile&quot;,&quot;tax revenue per capita&quot;,&quot;=1 if in western N.C.&quot;,&quot;=1 if in central N.C.&quot;,&quot;=1 if in SMSA&quot;,&quot;perc. minority, 1980&quot;,&quot;weekly wage, construction&quot;,&quot;wkly wge, trns, util, commun&quot;,&quot;wkly wge, whlesle, retail trade&quot;,&quot;wkly wge, fin, ins, real est&quot;,&quot;wkly wge, service industry&quot;,&quot;wkly wge, manufacturing&quot;,&quot;wkly wge, fed employees&quot;,&quot;wkly wge, state employees&quot;,&quot;wkly wge, local gov emps&quot;,&quot;offense mix: face-to-face/other&quot;,&quot;percent young male&quot;)
data_group &lt;- c(&quot;Control&quot;,&quot;&quot;,&quot;&quot;,&quot;Deterrent&quot;,&quot;Deterrent&quot;,&quot;Deterrent&quot;,&quot;Deterrent&quot;,&quot;Deterrent&quot;,&quot;Demographic&quot;,&quot;Demographic&quot;,&quot;Region&quot;,&quot;Region&quot;,&quot;Urban&quot;,&quot;Demographic&quot;,&quot;Wages&quot;,&quot;Wages&quot;,&quot;Wages&quot;,&quot;Wages&quot;,&quot;Wages&quot;,&quot;Wages&quot;,&quot;Wages&quot;,&quot;Wages&quot;,&quot;Wages&quot;,&quot;Demographic&quot;,&quot;Demographic&quot;)
data_notes &lt;- c(&quot;&quot;,&quot;&quot;,&quot;ratio of FBI index crimes to county population&quot;,&quot;ratio of arrests to offenses&quot;,&quot;ratio of convictions to arrests&quot;,&quot;proportion of total convictions resulting in prison sentences&quot;,&quot;average sentence in days&quot;,&quot;&quot;,&quot;country population divided by county land area&quot;,&quot;&quot;,&quot;dummy&quot;,&quot;dummy&quot;,&quot;dummy&quot;,&quot;proportion of country population that is minority or nonwhite&quot;,&quot;average weekly wage in that sector&quot;,&quot;average weekly wage in that sector&quot;,&quot;average weekly wage in that sector&quot;,&quot;average weekly wage in that sector&quot;,&quot;average weekly wage in that sector&quot;,&quot;average weekly wage in that sector&quot;,&quot;average weekly wage in that sector&quot;,&quot;average weekly wage in that sector&quot;,&quot;average weekly wage in that sector&quot;,&quot;ratio of face-to-face crimes (robbery, assault, rape) to non-face-to-face crimes&quot;,&quot;proportion of country population that is male between 15 and 24&quot;
)
data_headers &lt;- c(&quot;Variable&quot;, &quot;Description&quot;, &quot;Group&quot;, &quot;Note&quot;)
data_table &lt;- data.frame(data_variables, data_description, data_group, data_notes)
kable(data_table, col.names = data_headers, caption = &quot;Descriptions and Groups of Variables&quot;)</code></pre>
<table>
<caption>Descriptions and Groups of Variables</caption>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="left">Description</th>
<th align="left">Group</th>
<th align="left">Note</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">county</td>
<td align="left">county identifier</td>
<td align="left">Control</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">year</td>
<td align="left">1987</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">crmrte</td>
<td align="left">crimes committed per person</td>
<td align="left"></td>
<td align="left">ratio of FBI index crimes to county population</td>
</tr>
<tr class="even">
<td align="left">prbarr</td>
<td align="left">‘probability’ of arrest</td>
<td align="left">Deterrent</td>
<td align="left">ratio of arrests to offenses</td>
</tr>
<tr class="odd">
<td align="left">prbconv</td>
<td align="left">‘probability’ of conviction</td>
<td align="left">Deterrent</td>
<td align="left">ratio of convictions to arrests</td>
</tr>
<tr class="even">
<td align="left">prbpris</td>
<td align="left">‘probability’ of prison sentence</td>
<td align="left">Deterrent</td>
<td align="left">proportion of total convictions resulting in prison sentences</td>
</tr>
<tr class="odd">
<td align="left">avgsen</td>
<td align="left">avg. sentence, days</td>
<td align="left">Deterrent</td>
<td align="left">average sentence in days</td>
</tr>
<tr class="even">
<td align="left">polpc</td>
<td align="left">police per capita</td>
<td align="left">Deterrent</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">density</td>
<td align="left">people per sq. mile</td>
<td align="left">Demographic</td>
<td align="left">country population divided by county land area</td>
</tr>
<tr class="even">
<td align="left">taxpc</td>
<td align="left">tax revenue per capita</td>
<td align="left">Demographic</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">west</td>
<td align="left">=1 if in western N.C.</td>
<td align="left">Region</td>
<td align="left">dummy</td>
</tr>
<tr class="even">
<td align="left">central</td>
<td align="left">=1 if in central N.C.</td>
<td align="left">Region</td>
<td align="left">dummy</td>
</tr>
<tr class="odd">
<td align="left">urban</td>
<td align="left">=1 if in SMSA</td>
<td align="left">Urban</td>
<td align="left">dummy</td>
</tr>
<tr class="even">
<td align="left">pctmin80</td>
<td align="left">perc. minority, 1980</td>
<td align="left">Demographic</td>
<td align="left">proportion of country population that is minority or nonwhite</td>
</tr>
<tr class="odd">
<td align="left">wcon</td>
<td align="left">weekly wage, construction</td>
<td align="left">Wages</td>
<td align="left">average weekly wage in that sector</td>
</tr>
<tr class="even">
<td align="left">wtuc</td>
<td align="left">wkly wge, trns, util, commun</td>
<td align="left">Wages</td>
<td align="left">average weekly wage in that sector</td>
</tr>
<tr class="odd">
<td align="left">wtrd</td>
<td align="left">wkly wge, whlesle, retail trade</td>
<td align="left">Wages</td>
<td align="left">average weekly wage in that sector</td>
</tr>
<tr class="even">
<td align="left">wfir</td>
<td align="left">wkly wge, fin, ins, real est</td>
<td align="left">Wages</td>
<td align="left">average weekly wage in that sector</td>
</tr>
<tr class="odd">
<td align="left">wser</td>
<td align="left">wkly wge, service industry</td>
<td align="left">Wages</td>
<td align="left">average weekly wage in that sector</td>
</tr>
<tr class="even">
<td align="left">wmfg</td>
<td align="left">wkly wge, manufacturing</td>
<td align="left">Wages</td>
<td align="left">average weekly wage in that sector</td>
</tr>
<tr class="odd">
<td align="left">wfed</td>
<td align="left">wkly wge, fed employees</td>
<td align="left">Wages</td>
<td align="left">average weekly wage in that sector</td>
</tr>
<tr class="even">
<td align="left">wsta</td>
<td align="left">wkly wge, state employees</td>
<td align="left">Wages</td>
<td align="left">average weekly wage in that sector</td>
</tr>
<tr class="odd">
<td align="left">wloc</td>
<td align="left">wkly wge, local gov emps</td>
<td align="left">Wages</td>
<td align="left">average weekly wage in that sector</td>
</tr>
<tr class="even">
<td align="left">mix</td>
<td align="left">offense mix: face-to-face/other</td>
<td align="left">Demographic</td>
<td align="left">ratio of face-to-face crimes (robbery, assault, rape) to non-face-to-face crimes</td>
</tr>
<tr class="odd">
<td align="left">pctymle</td>
<td align="left">percent young male</td>
<td align="left">Demographic</td>
<td align="left">proportion of country population that is male between 15 and 24</td>
</tr>
</tbody>
</table>
<p>To get a better sense of the data set the summary function was run.</p>
<pre class="r"><code>summary(data)</code></pre>
<p>This function provides a high level view of each variable. Six rows have missing values for all variables. In addition, there is one duplicate row. Also the variable prbconv is loaded as a factor, so it needs to be converted to numeric. These issues are handled below to create the initial data set.</p>
<pre class="r"><code>#eliminate N/A&#39;s (6 rows of NA were removed)
data_crmrte &lt;- data[!is.na(data$crmrte),]

#remove duplicates (1 duplicate record was found)
data_crmrte &lt;- data_crmrte %&gt;% distinct()

#prbconv was defined as factor , we will convert it to numeric
data_crmrte$prbconv &lt;- as.numeric(as.character(data_crmrte$prbconv))
class(data_crmrte$prbconv)</code></pre>
<pre><code>## [1] &quot;numeric&quot;</code></pre>
<p>With 25 original variables in the data set the natural place to start is with the dependent variable, crmrte. To get a better sense of this variable, the distribution is graphed below.</p>
<pre class="r"><code>quantile(data_crmrte$crmrte, c(0, .01, .05, .10, .25, .50,  .75, .90, .95, .99, 1.0))</code></pre>
<pre><code>##         0%         1%         5%        10%        25%        50%        75% 
## 0.00553320 0.01006330 0.01235660 0.01418007 0.02060425 0.03000200 0.04024925 
##        90%        95%        99%       100% 
## 0.06054659 0.07191830 0.08954881 0.09896590</code></pre>
<pre class="r"><code>hist(data_crmrte$crmrte,breaks=20)
boxplot(data_crmrte$crmrte, main=&quot;Boxplot of crmrte&quot;)</code></pre>
<table><tr><td><img src="lab_3_master_files/figure-html/unnamed-chunk-6-1.png" width=".49\linewidth" style="display: block; margin: auto;" /></td><td><img src="lab_3_master_files/figure-html/unnamed-chunk-6-2.png" width=".49\linewidth" style="display: block; margin: auto;" /></td></tr></table>
<pre class="r"><code>plot(data_crmrte$crmrte)
qqnorm(data_crmrte$crmrte)
shapiro.test(data_crmrte$crmrte) # Shapiro-wilk test confirms non-normality</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  data_crmrte$crmrte
## W = 0.89162, p-value = 1.741e-06</code></pre>
<table><tr><td><img src="lab_3_master_files/figure-html/unnamed-chunk-7-1.png" width=".49\linewidth" style="display: block; margin: auto;" /></td><td><img src="lab_3_master_files/figure-html/unnamed-chunk-7-2.png" width=".49\linewidth" style="display: block; margin: auto;" /> # Outlier Analysis</td></tr></table>
<p><br />
There are several outliers in the variable crmrte and the distribution is right skewed. We have ninety observations so non-normality is not a top concern but this distribution is not perfectly normal. we analyse outliers for crime rate that are &gt; 2*Std-dev from the mean crime rate (i.e data pts with crime rate &gt; 0.07)</p>
<p>The postiviely skewed outliers (6 counties) on the right side of the distribution are examined to gather some insights: * 1. 4 of out of the 6 outliers are in urban areas * 2. The average demographic density for the outlier set is greater than 3 times the average density for the overall sample * 3. We also observe that data ppt 53 which has the highest crime rate, also has the highest density amongst the outliers and is a urban area</p>
<p>This is not very surprising as we expect urban areas with high density of population to have more crimes. we will continue to monitor the impact of the outliers and conisder the treatment of these outlier in a later part of the report.</p>
<pre class="r"><code>upper &lt;- data_crmrte[data_crmrte$crmrte &gt; 0.07,]
density_table &lt;- data.frame(upper$county, upper$crmrte, upper$density)
kable(density_table, col.names = c(&quot;County&quot;, &quot;Crime Rate&quot;, &quot;Density&quot;), 
      caption = &quot;Density and Outliers&quot;)</code></pre>
<table>
<caption>Density and Outliers</caption>
<thead>
<tr class="header">
<th align="right">County</th>
<th align="right">Crime Rate</th>
<th align="right">Density</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">51</td>
<td align="right">0.0883849</td>
<td align="right">3.9345510</td>
</tr>
<tr class="even">
<td align="right">55</td>
<td align="right">0.0790163</td>
<td align="right">0.5115089</td>
</tr>
<tr class="odd">
<td align="right">63</td>
<td align="right">0.0706599</td>
<td align="right">5.6744967</td>
</tr>
<tr class="even">
<td align="right">119</td>
<td align="right">0.0989659</td>
<td align="right">8.8276520</td>
</tr>
<tr class="odd">
<td align="right">129</td>
<td align="right">0.0834982</td>
<td align="right">6.2864866</td>
</tr>
<tr class="even">
<td align="right">181</td>
<td align="right">0.0729479</td>
<td align="right">1.5702811</td>
</tr>
</tbody>
</table>
<p>We also look at the lower range of outliers and find only data pt 51 (county 115) which has crime rate &lt; 0.01. This outlier has some significant outlier effects and will be explored further later in the report.<br />
</p>
<pre class="r"><code>lower &lt;- data_crmrte[data_crmrte$crmrte &lt; 0.01,]
density_table &lt;- data.frame(lower$county, lower$crmrte, lower$density)
kable(density_table, col.names = c(&quot;County&quot;, &quot;Crime Rate&quot;, &quot;Density&quot;), 
      caption = &quot;Density and Outliers&quot;)</code></pre>
<table>
<caption>Density and Outliers</caption>
<thead>
<tr class="header">
<th align="right">County</th>
<th align="right">Crime Rate</th>
<th align="right">Density</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">115</td>
<td align="right">0.0055332</td>
<td align="right">0.3858093</td>
</tr>
</tbody>
</table>
<p>For campaign purposes, we want to predict crime. We want our candidate to be able to say that he or she can reduce crime in order to win votes. What is the most effective way to convey that? Using crime rate as it appears in the data set is using the level of crime rate and would suggest the following statement as a campaign slogan - “I can reduce crime to this rate by doing x, y, and z”.</p>
<p>Transforming crime rate into the log of crime rate allows for the statement “I can reduce crime by n% by doing x, y, and z.” We find the latter more powerful and meaningful to voters since voters have no idea about the level of crime rates. In addition, we will show that the transformation of crime rate imrpoves the normaility and distribution of the variable, which will often reduce skew in the errors as well.<br />
</p>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  data_crmrte$log_crmrte
## W = 0.98857, p-value = 0.626</code></pre>
<table><tr><td><img src="lab_3_master_files/figure-html/unnamed-chunk-10-1.png" width=".49\linewidth" style="display: block; margin: auto;" /></td><td><img src="lab_3_master_files/figure-html/unnamed-chunk-10-2.png" width=".49\linewidth" style="display: block; margin: auto;" /></td></tr>
  <tr><td><img src="lab_3_master_files/figure-html/unnamed-chunk-10-3.png" width=".49\linewidth" style="display: block; margin: auto;" /></td><td><img src="lab_3_master_files/figure-html/unnamed-chunk-10-4.png" width=".49\linewidth" style="display: block; margin: auto;" /> </td></tr></table>
<p>
  The histogram of the transformed crime rate is much more symmetrical and shows much less right skew. The box plot shows all of the outliers on the high end have been removed, though outlier 51 ( countty 115) on the low end has been become more prominent.</p>
<p>The scatter plot looks much more normal, and the Q-Q plot is much closer to normal with the data points hugging the 45 degree line much more closely. Given the stronger argument for the political campaign and the benefits to normality we have chosen to model the tranformation of crime rate as opposed to crime rate.<br />
</p>
</div>
<div id="groupings" class="section level1">
<h3>Groupings</h3>
<p>In order to digest the data in the data set we decided to group the variables into five groups: deterrent, wages, demographic, region, and urban. We performed exploratory data analysis on all of these variables.<br />
</p>
<p>The first group is deterrent data. As cited in the original paper, these variables were hypothesized to reduce crime rate through disincentivizing crime. Essentially, as the probability of getting caught increases, criminals’ desire to commit crimes decreases.<br />
</p>
</div>
<div id="deterrent-data" class="section level1">
<h3>Deterrent Data</h3>
<pre class="r"><code>deterrent_data &lt;- data_crmrte[,c(&#39;prbarr&#39;,&#39;prbconv&#39;,&#39;prbpris&#39;,&#39;crmrte&#39;,
                          &#39;avgsen&#39;,&#39;polpc&#39;)]

ggplot(gather(deterrent_data[,c(&#39;prbarr&#39;,&#39;prbconv&#39;,&#39;prbpris&#39;,
                                    &#39;avgsen&#39;,&#39;polpc&#39;)]), aes(value)) + 
geom_histogram(bins = 10) + facet_wrap(~key, scales = &#39;free_x&#39;)</code></pre>
<p><img src="lab_3_master_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>my_vars1 &lt;- c(&quot;prbarr&quot;,&quot;prbconv&quot;,&quot;prbpris&quot;)
deterrent_data2 &lt;- deterrent_data[my_vars1]
my_vars2 &lt;- c(&quot;polpc&quot;)
deterrent_data3 &lt;- deterrent_data[my_vars2]

boxplot(deterrent_data2, main=&quot;Boxplot of prbarr, prbconv, prbpris&quot;)</code></pre>
<p><img src="lab_3_master_files/figure-html/unnamed-chunk-11-2.png" width="672" /><br />
The first four histograms show right skew while prbpris shows left skew. The biggest outlier is observation 51. This observation has the lowest crime rate in the data set, the highest polpc (police per capita), the highest avg sentence, the third highest prbconv, and the lowest pctmin80. This observation is likely to affect many of the regressions so it will need to be examined further. These variables are candidates to be transformed.<br />
</p>
</div>
<div id="wages-data" class="section level1">
<h3>Wages Data</h3>
<pre class="r"><code>#create a dataframe of just the wage variables
wages_data &lt;- data_crmrte[,c(&#39;wcon&#39;,&#39;wtuc&#39;,&#39;wtrd&#39;,&#39;wfir&#39;, &#39;wser&#39;,
                      &#39;wmfg&#39;,&#39;wfed&#39;, &#39;wsta&#39;, &#39;wloc&#39;)]

#plot histograms of just the wage variables
ggplot(gather(wages_data), aes(value)) + 
  geom_histogram(bins = 10) + 
  facet_wrap(~key, scales = &#39;free_x&#39;)</code></pre>
<p><img src="lab_3_master_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>#generate boxplots of just the wage variables
boxplot(log(wages_data), ylab="log(wage)", xlab="variable")</code></pre>
<p><img src="000004.png" width="672" /><br />
There is an obvious outlier for wser in data pt 84 (County 185) . The mean services wage across all the counties is $275 ( with a std dev of 206) and 84 has wser of 2177 (~9sd from mean), which seems like a measurement or typographical error. The next highest average weekly wage in any sector is 646 versus the value of 2177.It is very possible that this data point might add measurement error and we will revisit this later.</p>
<p>For now, we create an additional variable that is the median of all wage variables for each observation. If it conveys as much information, it has the benefit of increasing our degress of freedom and removing the effect of the outlier.<br />
</p>
<pre class="r"><code>data_crmrte$median_wage &lt;- apply(data_crmrte[c(&quot;wcon&quot;, &quot;wtuc&quot;, &quot;wtrd&quot;,
                                               &quot;wfir&quot;, &quot;wser&quot;, &quot;wmfg&quot;,
                                               &quot;wfed&quot;, &quot;wsta&quot;, &quot;wloc&quot;)], 
                                 1, FUN=median, na.rm=TRUE)</code></pre>
</div>
<div id="region-data" class="section level1">
<h3>Region Data</h3>
<pre class="r"><code>#create a dataframe of just the wage variables
dummies_data &lt;- data_crmrte[,c(&#39;west&#39;,&#39;central&#39;)]

#plot histograms of just the dummy variables
ggplot(gather(dummies_data), aes(value)) + 
  geom_histogram(bins = 2) + 
  facet_wrap(~key)</code></pre>
<p><img src="lab_3_master_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>#just a quick check that there is no overlap
region_check &lt;- data_crmrte[which(data_crmrte$west == 1 &amp;&amp; data_crmrte$central == 1)]

summary(region_check)</code></pre>
<pre><code>## &lt; table of extent 0 x 0 &gt;</code></pre>
<p>The regions are broken up into central, west, and east. East is left out of the data set and it’s effect as the final level of the indicator variable will move to the intercept.<br />
</p>
</div>
<div id="urban-data" class="section level1">
<h3>Urban Data</h3>
<pre class="r"><code>#plot histograms of just the wage variables
sum(data_crmrte$urban) # There are only 8 Urban areas out of 90 counties</code></pre>
<pre><code>## [1] 8</code></pre>
<pre class="r"><code>hist(data_crmrte$urban)</code></pre>
<p><img src="lab_3_master_files/figure-html/unnamed-chunk-16-1.png" width="672" /> Urban did not fit into a great grouping so we left this variable on its own. A histogram shows that the state has relatively few urban counties, something to keep in mind when analyzing other variables such as density.<br />
</p>
</div>
<div id="demographic-data" class="section level1">
<h3>Demographic Data</h3>
<pre class="r"><code>#create a dataframe of just the demographic variables
demographic_data &lt;- data_crmrte[,c(&#39;density&#39;, &#39;taxpc&#39;, &#39;pctmin80&#39;,
                                   &#39;mix&#39;, &#39;pctymle&#39;)]

#plot histograms of just the demographic variables
ggplot(gather(demographic_data), aes(value)) + 
  geom_histogram(bins = 10) + 
  facet_wrap(~key, scales = &#39;free_x&#39;)</code></pre>
<p><img src="lab_3_master_files/figure-html/unnamed-chunk-17-1.png" width="672" /> Once again we see a lot of right skewed distributions in the historgams and in the box plots.<br />
</p>
<p>After exploring all of the variables we decided to tranform the other variables that are potentially under a politican’s control - the deterrent variables. This gives us our final data set and so we can start running regressions.<br />
</p>
<pre class="r"><code>data_crmrte$prbconv &lt;- as.numeric(as.character(data_crmrte$prbconv))
data_crmrte$log_prbarr &lt;- log(data_crmrte$prbarr)
data_crmrte$log_prbconv &lt;- log(data_crmrte$prbconv)
data_crmrte$log_prbpris &lt;- log(data_crmrte$prbpris)
data_crmrte$log_avgsen &lt;- log(data_crmrte$avgsen)
data_crmrte$log_polpc &lt;- log(data_crmrte$polpc)
data_crmrte$log_taxpc &lt;- log(data_crmrte$taxpc)


#plot histograms of just the demographic variables
ggplot(gather(data_crmrte[,c(&#39;log_prbarr&#39;, &#39;log_prbconv&#39;, &#39;log_prbpris&#39;, &#39;log_avgsen&#39;, &#39;log_polpc&#39;, &#39;log_taxpc&#39;) ]), aes(value)) + 
  geom_histogram(bins = 10) + 
  facet_wrap(~key, scales = &#39;free_x&#39;)</code></pre>
<p><img src="lab_3_master_files/figure-html/unnamed-chunk-18-1.png" width="672" /><br />
Though the distribution of the variables still exhibits skew, the skew does seem to be reduced.</p>
</div>
<div id="log-tranformed-dependent-variable-comparison" class="section level1">
<h3>Log Tranformed Dependent Variable Comparison</h3>
<p><br />
In order to settle on the final data set we compare an all-in log-log model with an all-in log-linear to see which dependent variables are more suitable.<br />
</p>
<pre class="r"><code>###### Initial Models #####
all_in_model &lt;- lm(crmrte ~ prbarr + prbconv + prbpris 
                   + avgsen + polpc + density
                   + taxpc + west + central + urban + pctmin80 + wcon
                   + wtuc + wtrd + wfir + wser + wmfg 
                   + wfed + wsta + wloc
                   + mix + pctymle,
                   data = data_crmrte)
se.all_in_model = sqrt(diag(vcovHC(all_in_model)))
coeftest(all_in_model, vcov = vcovHC)</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                Estimate  Std. Error t value  Pr(&gt;|t|)    
## (Intercept)  1.3853e-02  3.0755e-02  0.4504 0.6538622    
## prbarr      -5.1466e-02  1.5689e-02 -3.2805 0.0016467 ** 
## prbconv     -1.8633e-02  6.5853e-03 -2.8295 0.0061464 ** 
## prbpris      3.1727e-03  1.3586e-02  0.2335 0.8160642    
## avgsen      -3.9858e-04  5.5361e-04 -0.7200 0.4740570    
## polpc        6.9679e+00  2.9536e+00  2.3591 0.0212406 *  
## density      5.3314e-03  1.4895e-03  3.5793 0.0006464 ***
## taxpc        1.6240e-04  2.8408e-04  0.5717 0.5694537    
## west        -2.5652e-03  4.4698e-03 -0.5739 0.5679579    
## central     -4.2416e-03  3.7423e-03 -1.1334 0.2610725    
## urban       -9.6498e-05  8.2752e-03 -0.0117 0.9907307    
## pctmin80     3.2542e-04  1.3849e-04  2.3497 0.0217429 *  
## wcon         2.3025e-05  3.2876e-05  0.7004 0.4861334    
## wtuc         6.1914e-06  1.9862e-05  0.3117 0.7562178    
## wtrd         2.8767e-05  8.7294e-05  0.3295 0.7427756    
## wfir        -3.5455e-05  3.5699e-05 -0.9932 0.3242068    
## wser        -1.7158e-06  9.9447e-05 -0.0173 0.9862856    
## wmfg        -8.9675e-06  1.7469e-05 -0.5133 0.6094087    
## wfed         2.9075e-05  3.7780e-05  0.7696 0.4442480    
## wsta        -2.2302e-05  3.6828e-05 -0.6056 0.5468431    
## wloc         1.4456e-05  8.5367e-05  0.1693 0.8660410    
## mix         -1.8693e-02  2.2922e-02 -0.8155 0.4176761    
## pctymle      1.0125e-01  4.7826e-02  2.1170 0.0379748 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>AIC(all_in_model)</code></pre>
<pre><code>## [1] -585.5858</code></pre>
<pre class="r"><code>all_in_model_log_level &lt;- lm(log_crmrte ~ prbarr + prbconv + prbpris 
                             + avgsen + polpc + density
                             + taxpc + west + central + urban 
                             + pctmin80 + wcon
                             + wtuc + wtrd + wfir + wser + wmfg 
                             + wfed + wsta + wloc
                             + mix + pctymle,
                             data = data_crmrte)
se.all_in_model_log_level = sqrt(diag(vcovHC(all_in_model_log_level)))
coeftest(all_in_model_log_level, vcov = vcovHC)</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                Estimate  Std. Error t value  Pr(&gt;|t|)    
## (Intercept) -4.0261e+00  8.4822e-01 -4.7466 1.128e-05 ***
## prbarr      -1.8891e+00  3.7955e-01 -4.9773 4.770e-06 ***
## prbconv     -6.5603e-01  1.7443e-01 -3.7611 0.0003579 ***
## prbpris     -9.3077e-02  3.9921e-01 -0.2332 0.8163542    
## avgsen      -7.8769e-03  1.6125e-02 -0.4885 0.6267962    
## polpc        1.5484e+02  8.6523e+01  1.7895 0.0780510 .  
## density      1.1653e-01  5.4037e-02  2.1566 0.0346326 *  
## taxpc        3.3224e-03  7.2890e-03  0.4558 0.6500012    
## west        -1.1492e-01  1.2509e-01 -0.9187 0.3615403    
## central     -1.0078e-01  9.2053e-02 -1.0948 0.2775232    
## urban       -1.6923e-01  2.2872e-01 -0.7399 0.4619535    
## pctmin80     9.9770e-03  3.0480e-03  3.2733 0.0016833 ** 
## wcon         4.6001e-04  8.3564e-04  0.5505 0.5838140    
## wtuc         1.0174e-04  6.0187e-04  0.1690 0.8662750    
## wtrd         2.5964e-04  1.7638e-03  0.1472 0.8834136    
## wfir        -1.1015e-03  1.1960e-03 -0.9210 0.3603557    
## wser        -1.3142e-04  1.5060e-03 -0.0873 0.9307193    
## wmfg        -2.0528e-04  5.1630e-04 -0.3976 0.6921878    
## wfed         2.3405e-03  1.0820e-03  2.1632 0.0340968 *  
## wsta        -1.1357e-03  8.9769e-04 -1.2651 0.2102213    
## wloc         5.8983e-04  2.4003e-03  0.2457 0.8066400    
## mix         -2.3924e-01  6.2632e-01 -0.3820 0.7036869    
## pctymle      2.7706e+00  1.4330e+00  1.9334 0.0574191 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>AIC(all_in_model_log_level)</code></pre>
<pre><code>## [1] 21.354</code></pre>
<pre class="r"><code>all_in_model_log_log &lt;- lm(log_crmrte ~ log_prbarr + log_prbconv 
                             + log_prbpris + log_avgsen + log_polpc
                             + density+ log_taxpc + west + central 
                             + urban + pctmin80 + wcon
                             + wtuc + wtrd + wfir 
                             + wser + wmfg + wfed + wsta + wloc
                             + mix + pctymle,
                             data = data_crmrte)
se.all_in_model_log_log = sqrt(diag(vcovHC(all_in_model_log_log)))
coeftest(all_in_model_log_log, vcov = vcovHC)</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                Estimate  Std. Error t value Pr(&gt;|t|)   
## (Intercept) -3.36882669  2.97497990 -1.1324 0.261508   
## log_prbarr  -0.52143620  0.16459898 -3.1679 0.002313 **
## log_prbconv -0.33101341  0.15365522 -2.1543 0.034820 * 
## log_prbpris -0.06569465  0.19741379 -0.3328 0.740342   
## log_avgsen  -0.19652151  0.18205821 -1.0794 0.284261   
## log_polpc    0.29132794  0.27176129  1.0720 0.287567   
## density      0.12320127  0.06040422  2.0396 0.045335 * 
## log_taxpc    0.06158051  0.30979897  0.1988 0.843040   
## west        -0.18453792  0.16353910 -1.1284 0.263174   
## central     -0.10789292  0.09991865 -1.0798 0.284100   
## urban       -0.14767055  0.26670745 -0.5537 0.581641   
## pctmin80     0.00956927  0.00358175  2.6717 0.009466 **
## wcon         0.00078953  0.00090745  0.8701 0.387376   
## wtuc         0.00010106  0.00075559  0.1337 0.894001   
## wtrd         0.00029022  0.00177967  0.1631 0.870952   
## wfir        -0.00108230  0.00125937 -0.8594 0.393186   
## wser        -0.00042887  0.00096365 -0.4451 0.657718   
## wmfg        -0.00014147  0.00061356 -0.2306 0.818343   
## wfed         0.00224918  0.00136611  1.6464 0.104363   
## wsta        -0.00102039  0.00106131 -0.9614 0.339787   
## wloc         0.00017815  0.00261968  0.0680 0.945986   
## mix         -0.44834658  0.77846459 -0.5759 0.566587   
## pctymle      2.00755501  2.60186976  0.7716 0.443075   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>AIC(all_in_model_log_log)</code></pre>
<pre><code>## [1] 44.17803</code></pre>
<pre class="r"><code>BIC(all_in_model_log_log)</code></pre>
<pre><code>## [1] 104.1735</code></pre>
<pre class="r"><code>#Not comparing r-squared, just looking at significant variables
stargazer(all_in_model, all_in_model_log_level, 
          all_in_model_log_log,
          type = &quot;text&quot;, omit.stat = &quot;f&quot;,
          se = list(se.all_in_model, se.all_in_model_log_level,
                    se.all_in_model_log_log),
          star.cutoffs = c(0.05, 0.01, 0.001))</code></pre>
<pre><code>## 
## =============================================================
##                                     Dependent variable:      
##                               -------------------------------
##                                 crmrte        log_crmrte     
##                                  (1)        (2)        (3)   
## -------------------------------------------------------------
## prbarr                         -0.051**  -1.889***           
##                                (0.016)    (0.380)            
##                                                              
## prbconv                        -0.019**  -0.656***           
##                                (0.007)    (0.174)            
##                                                              
## prbpris                         0.003      -0.093            
##                                (0.014)    (0.399)            
##                                                              
## avgsen                         -0.0004     -0.008            
##                                (0.001)    (0.016)            
##                                                              
## polpc                           6.968*    154.835            
##                                (2.954)    (86.523)           
##                                                              
## log_prbarr                                          -0.521** 
##                                                      (0.165) 
##                                                              
## log_prbconv                                          -0.331* 
##                                                      (0.154) 
##                                                              
## log_prbpris                                          -0.066  
##                                                      (0.197) 
##                                                              
## log_avgsen                                           -0.197  
##                                                      (0.182) 
##                                                              
## log_polpc                                             0.291  
##                                                      (0.272) 
##                                                              
## density                        0.005***    0.117*    0.123*  
##                                (0.001)    (0.054)    (0.060) 
##                                                              
## taxpc                           0.0002     0.003             
##                                (0.0003)   (0.007)            
##                                                              
## log_taxpc                                             0.062  
##                                                      (0.310) 
##                                                              
## west                            -0.003     -0.115    -0.185  
##                                (0.004)    (0.125)    (0.164) 
##                                                              
## central                         -0.004     -0.101    -0.108  
##                                (0.004)    (0.092)    (0.100) 
##                                                              
## urban                          -0.0001     -0.169    -0.148  
##                                (0.008)    (0.229)    (0.267) 
##                                                              
## pctmin80                       0.0003*    0.010**    0.010** 
##                                (0.0001)   (0.003)    (0.004) 
##                                                              
## wcon                           0.00002     0.0005     0.001  
##                               (0.00003)   (0.001)    (0.001) 
##                                                              
## wtuc                           0.00001     0.0001    0.0001  
##                               (0.00002)   (0.001)    (0.001) 
##                                                              
## wtrd                           0.00003     0.0003    0.0003  
##                                (0.0001)   (0.002)    (0.002) 
##                                                              
## wfir                           -0.00004    -0.001    -0.001  
##                               (0.00004)   (0.001)    (0.001) 
##                                                              
## wser                           -0.00000   -0.0001    -0.0004 
##                                (0.0001)   (0.002)    (0.001) 
##                                                              
## wmfg                           -0.00001   -0.0002    -0.0001 
##                               (0.00002)   (0.001)    (0.001) 
##                                                              
## wfed                           0.00003     0.002*     0.002  
##                               (0.00004)   (0.001)    (0.001) 
##                                                              
## wsta                           -0.00002    -0.001    -0.001  
##                               (0.00004)   (0.001)    (0.001) 
##                                                              
## wloc                           0.00001     0.001     0.0002  
##                                (0.0001)   (0.002)    (0.003) 
##                                                              
## mix                             -0.019     -0.239    -0.448  
##                                (0.023)    (0.626)    (0.778) 
##                                                              
## pctymle                         0.101*     2.771      2.008  
##                                (0.048)    (1.433)    (2.602) 
##                                                              
## Constant                        0.014    -4.026***   -3.369  
##                                (0.031)    (0.848)    (2.975) 
##                                                              
## -------------------------------------------------------------
## Observations                      90         90        90    
## R2                              0.855      0.854      0.812  
## Adjusted R2                     0.807      0.806      0.750  
## Residual Std. Error (df = 67)   0.008      0.242      0.275  
## =============================================================
## Note:                           *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001</code></pre>
</div>
<div id="model-1-simple-model" class="section level1">
<h3>Model 1: Simple Model</h3>
<p>In order to create a simple model we decided to build using a bottom up approach. We looked at a correlation matrix</p>
<pre class="r"><code>cm = round(cor(data_crmrte$log_crmrte,data_crmrte)*100,2) #Corr Matrix as % for reading clarity
cm = cm[, -1]
print(cm)</code></pre>
<pre><code>##        year      crmrte      prbarr     prbconv     prbpris      avgsen 
##          NA       94.15      -47.28      -44.68        2.15       -4.94 
##       polpc     density       taxpc        west     central       urban 
##        1.04       63.30       35.83      -41.44       18.47       49.15 
##    pctmin80        wcon        wtuc        wtrd        wfir        wser 
##       23.29       39.37       20.15       39.38       29.32      -11.31 
##        wmfg        wfed        wsta        wloc         mix     pctymle 
##       30.75       52.33       16.97       28.86      -12.47       27.82 
##  log_crmrte median_wage  log_prbarr log_prbconv log_prbpris  log_avgsen 
##      100.00       45.44      -43.58      -37.25        6.96        2.34 
##   log_polpc   log_taxpc 
##       28.45       33.98</code></pre>
<p>In the above correlation matrix, focusing on the correlations between the log_crmrte and all other variables, denisty has the highest correlation. This variable makes intuitive sense. As a single variable it might encompass a lot of other factors. Lower income people with more incentive to commit crimes tend to live in more highly populated areas. Below is the simple regression.</p>
<pre class="r"><code>simple_regression_model &lt;- lm(log_crmrte ~ density, data = data_crmrte)
se.simple_regression_model = sqrt(diag(vcovHC(simple_regression_model)))
coeftest(simple_regression_model, vcov = vcovHC)</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error  t value Pr(&gt;|t|)    
## (Intercept) -3.869488   0.068563 -56.4366  &lt; 2e-16 ***
## density      0.228298   0.030439   7.5003  4.8e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>AIC(simple_regression_model)</code></pre>
<pre><code>## [1] 106.2991</code></pre>
<pre class="r"><code>stargazer(simple_regression_model,
          type = &quot;text&quot;, omit.stat = &quot;f&quot;,
          se = list(se.simple_regression_model),
          star.cutoffs = c(0.05, 0.01, 0.001))</code></pre>
<pre><code>## 
## =================================================
##                          Dependent variable:     
##                     -----------------------------
##                              log_crmrte          
## -------------------------------------------------
## density                       0.228***           
##                                (0.030)           
##                                                  
## Constant                      -3.869***          
##                                (0.069)           
##                                                  
## -------------------------------------------------
## Observations                     90              
## R2                              0.401            
## Adjusted R2                     0.394            
## Residual Std. Error        0.427 (df = 88)       
## =================================================
## Note:               *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001</code></pre>
<p>The variable density explains 40.1% of the variation in the log of crime rate. As density increases by 1 unit (as the county population divided by the county land area increases by 1%) crime increases by 22%.</p>
</div>
<div id="model-2-kitchen-sink-model" class="section level1">
<h3>Model 2: Kitchen Sink Model</h3>
<p>Still, we can do better in predicting the log crime rate than simply using one variable. We know examine a “kitchen sink” model. This model includes all of the variables in the data set except county (which has too many values to be a useful indicator variable) and year, which is a constant (1987). Below are the results.</p>
<pre class="r"><code>all_in_model_log_level &lt;- lm(log_crmrte ~ prbarr + prbconv + prbpris 
                             + avgsen + polpc + density
                             + taxpc + west + central + urban 
                             + pctmin80 + wcon
                             + wtuc + wtrd + wfir + wser + wmfg 
                             + wfed + wsta + wloc
                             + mix + pctymle,
                             data = data_crmrte)
se.all_in_model_log_level = sqrt(diag(vcovHC(all_in_model_log_level))) #HC White SE
coeftest(all_in_model_log_level, vcov = vcovHC) </code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                Estimate  Std. Error t value  Pr(&gt;|t|)    
## (Intercept) -4.0261e+00  8.4822e-01 -4.7466 1.128e-05 ***
## prbarr      -1.8891e+00  3.7955e-01 -4.9773 4.770e-06 ***
## prbconv     -6.5603e-01  1.7443e-01 -3.7611 0.0003579 ***
## prbpris     -9.3077e-02  3.9921e-01 -0.2332 0.8163542    
## avgsen      -7.8769e-03  1.6125e-02 -0.4885 0.6267962    
## polpc        1.5484e+02  8.6523e+01  1.7895 0.0780510 .  
## density      1.1653e-01  5.4037e-02  2.1566 0.0346326 *  
## taxpc        3.3224e-03  7.2890e-03  0.4558 0.6500012    
## west        -1.1492e-01  1.2509e-01 -0.9187 0.3615403    
## central     -1.0078e-01  9.2053e-02 -1.0948 0.2775232    
## urban       -1.6923e-01  2.2872e-01 -0.7399 0.4619535    
## pctmin80     9.9770e-03  3.0480e-03  3.2733 0.0016833 ** 
## wcon         4.6001e-04  8.3564e-04  0.5505 0.5838140    
## wtuc         1.0174e-04  6.0187e-04  0.1690 0.8662750    
## wtrd         2.5964e-04  1.7638e-03  0.1472 0.8834136    
## wfir        -1.1015e-03  1.1960e-03 -0.9210 0.3603557    
## wser        -1.3142e-04  1.5060e-03 -0.0873 0.9307193    
## wmfg        -2.0528e-04  5.1630e-04 -0.3976 0.6921878    
## wfed         2.3405e-03  1.0820e-03  2.1632 0.0340968 *  
## wsta        -1.1357e-03  8.9769e-04 -1.2651 0.2102213    
## wloc         5.8983e-04  2.4003e-03  0.2457 0.8066400    
## mix         -2.3924e-01  6.2632e-01 -0.3820 0.7036869    
## pctymle      2.7706e+00  1.4330e+00  1.9334 0.0574191 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>AIC(all_in_model_log_level)</code></pre>
<pre><code>## [1] 21.354</code></pre>
<pre class="r"><code>stargazer(simple_regression_model, all_in_model_log_level, 
          type = &quot;text&quot;, omit.stat = &quot;f&quot;,
          se = list(se.simple_regression_model, se.all_in_model_log_level),
          star.cutoffs = c(0.05, 0.01, 0.001))</code></pre>
<pre><code>## 
## ===================================================
##                           Dependent variable:      
##                     -------------------------------
##                               log_crmrte           
##                           (1)             (2)      
## ---------------------------------------------------
## prbarr                                 -1.889***   
##                                         (0.380)    
##                                                    
## prbconv                                -0.656***   
##                                         (0.174)    
##                                                    
## prbpris                                 -0.093     
##                                         (0.399)    
##                                                    
## avgsen                                  -0.008     
##                                         (0.016)    
##                                                    
## polpc                                   154.835    
##                                        (86.523)    
##                                                    
## density                0.228***         0.117*     
##                         (0.030)         (0.054)    
##                                                    
## taxpc                                    0.003     
##                                         (0.007)    
##                                                    
## west                                    -0.115     
##                                         (0.125)    
##                                                    
## central                                 -0.101     
##                                         (0.092)    
##                                                    
## urban                                   -0.169     
##                                         (0.229)    
##                                                    
## pctmin80                                0.010**    
##                                         (0.003)    
##                                                    
## wcon                                    0.0005     
##                                         (0.001)    
##                                                    
## wtuc                                    0.0001     
##                                         (0.001)    
##                                                    
## wtrd                                    0.0003     
##                                         (0.002)    
##                                                    
## wfir                                    -0.001     
##                                         (0.001)    
##                                                    
## wser                                    -0.0001    
##                                         (0.002)    
##                                                    
## wmfg                                    -0.0002    
##                                         (0.001)    
##                                                    
## wfed                                    0.002*     
##                                         (0.001)    
##                                                    
## wsta                                    -0.001     
##                                         (0.001)    
##                                                    
## wloc                                     0.001     
##                                         (0.002)    
##                                                    
## mix                                     -0.239     
##                                         (0.626)    
##                                                    
## pctymle                                  2.771     
##                                         (1.433)    
##                                                    
## Constant               -3.869***       -4.026***   
##                         (0.069)         (0.848)    
##                                                    
## ---------------------------------------------------
## Observations              90              90       
## R2                       0.401           0.854     
## Adjusted R2              0.394           0.806     
## Residual Std. Error 0.427 (df = 88) 0.242 (df = 67)
## ===================================================
## Note:                 *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001</code></pre>
<p>Unsuprisingly, the r-squared of the “kitchen sink” model is substantially higher (85.4% vs. 40.1%). More importantly, the adjusted r-squared which accounts for the number of variables in the models, is also higher (80.6% vs 39.4%). Interestingly, density is no longer the variable with the highest statistical significance. The coefficients show the effect after all the other variables have been controlled for (partialled out). In the “kitchen sink” model prbarr and prbconv both have the lowest p-values.<br />
</p>
</div>
<div id="model-3-balanced-model" class="section level1">
<h3>Model 3: Balanced Model</h3>
<p>We took two approaches to building the balanced model. We used a bottom up approach that relied on both the correlation matrix and stepwide regression. We also used a top down approach that started with the “kitchen sink” model and excluded variables. Both methods are discussed below. Both approaches relied on our categories of variables to simplify the process.<br />
</p>
<pre class="r"><code>base_forward = lm(log_crmrte ~ density,
                             data = data_crmrte)
forward_step = step(base_forward, scope = formula(all_in_model_log_level), direction = &quot;forward&quot;)</code></pre>
<p>With the top down approach, we started with model 3 and looked to exclude variables that weren’t as predictive. We ran hypothesis testing on all five groups, one group at a time.</p>
<pre class="r"><code>#deterrent
linearHypothesis(all_in_model_log_level, 
                 c(&quot;prbarr = 0&quot;, &quot;prbconv = 0&quot;, &quot;prbpris = 0&quot;,
                   &quot;avgsen = 0&quot;, &quot;polpc = 0&quot;), 
                 vcov = vcovHC)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"72","2":"NA","3":"NA","4":"NA","_rn_":"1"},{"1":"67","2":"5","3":"6.058218","4":"0.0001100825","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>#wage
linearHypothesis(all_in_model_log_level, 
                 c(&quot;wcon = 0&quot;, &quot;wtuc = 0&quot;, &quot;wtrd = 0&quot;,
                   &quot;wfir = 0&quot;, &quot;wser = 0&quot;, &quot;wmfg = 0&quot;,
                   &quot;wfed = 0&quot;, &quot;wsta = 0&quot;, &quot;wloc = 0&quot;), 
                 vcov = vcovHC)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"76","2":"NA","3":"NA","4":"NA","_rn_":"1"},{"1":"67","2":"9","3":"1.37203","4":"0.2184701","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>#region
linearHypothesis(all_in_model_log_level, 
                 c(&quot;west = 0&quot;, &quot;central = 0&quot;), 
                 vcov = vcovHC)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"69","2":"NA","3":"NA","4":"NA","_rn_":"1"},{"1":"67","2":"2","3":"0.6229584","4":"0.5394328","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>#urban
linearHypothesis(all_in_model_log_level, 
                 c(&quot;urban = 0&quot;), 
                 vcov = vcovHC)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"68","2":"NA","3":"NA","4":"NA","_rn_":"1"},{"1":"67","2":"1","3":"0.5474323","4":"0.4619535","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>#demographic
linearHypothesis(all_in_model_log_level, 
                 c(&quot;density = 0&quot;, &quot;taxpc = 0&quot;, &quot;pctmin80 = 0&quot;,
                   &quot;mix = 0&quot;, &quot;pctymle = 0&quot;), 
                 vcov = vcovHC)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"72","2":"NA","3":"NA","4":"NA","_rn_":"1"},{"1":"67","2":"5","3":"3.962659","4":"0.003298359","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The hypothesis tests below show that of the five groups the only groups that are jointly significant are the deterrent data and the demographic data. These tests measure whether removing all the variables within a group reduces the r-squared by statistically signficant amount. We will re-run the models and compare.</p>
<pre class="r"><code>balanced_model_top_1 &lt;- lm(log_crmrte ~ prbarr + prbconv + prbpris 
                             + avgsen + polpc + density
                             + taxpc + pctmin80 + mix + pctymle,
                             data = data_crmrte)
se.balanced_model_top_1 = sqrt(diag(vcovHC(balanced_model_top_1)))
coeftest(balanced_model_top_1, vcov = vcovHC)</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                Estimate  Std. Error t value  Pr(&gt;|t|)    
## (Intercept)  -3.3522918   0.3558217 -9.4213 1.467e-14 ***
## prbarr       -1.9627484   0.4014808 -4.8888 5.228e-06 ***
## prbconv      -0.7672158   0.1366862 -5.6130 2.846e-07 ***
## prbpris      -0.0764993   0.4732818 -0.1616  0.872005    
## avgsen       -0.0044749   0.0140406 -0.3187  0.750789    
## polpc       176.1347220  82.5884550  2.1327  0.036056 *  
## density       0.1135225   0.0351279  3.2317  0.001796 ** 
## taxpc         0.0020988   0.0055753  0.3764  0.707593    
## pctmin80      0.0125062   0.0016215  7.7128 3.155e-11 ***
## mix          -0.7304967   0.5396416 -1.3537  0.179702    
## pctymle       1.3832565   1.6211791  0.8532  0.396105    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>AIC(balanced_model_top_1)</code></pre>
<pre><code>## [1] 27.4514</code></pre>
<p>Our adjusted r-squared has only fallen from 80.6% to 77.6% but we have dropped 12 variables. This is a much more parisimous model. In order to double check wages, we decided to try to one more model that included just the median wage from all industries. The fundamental concept behind this is that the median could capture all opportunity for potential criminals, and it has the benefit of not being affected by the outlier in wser.</p>
<p>RESULT: Unfortunately, though it was much better, it was still not predictive.</p>
<pre class="r"><code>balanced_model_top_2 &lt;- lm(log_crmrte ~ prbarr + prbconv + prbpris 
                             + avgsen + polpc + density
                             + taxpc + pctmin80 + mix + pctymle
                             + median_wage,
                             data = data_crmrte)
se.balanced_model_top_2 = sqrt(diag(vcovHC(balanced_model_top_2)))
coeftest(balanced_model_top_2, vcov = vcovHC)</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                Estimate  Std. Error t value  Pr(&gt;|t|)    
## (Intercept)  -3.7018614   0.6241512 -5.9310 7.799e-08 ***
## prbarr       -1.9379154   0.4176289 -4.6403 1.381e-05 ***
## prbconv      -0.7650603   0.1360985 -5.6214 2.826e-07 ***
## prbpris      -0.1114352   0.4509889 -0.2471  0.805487    
## avgsen       -0.0046181   0.0137443 -0.3360  0.737770    
## polpc       167.9127417  85.2311288  1.9701  0.052378 .  
## density       0.0977596   0.0343625  2.8450  0.005671 ** 
## taxpc         0.0020705   0.0056750  0.3648  0.716214    
## pctmin80      0.0123893   0.0016202  7.6467 4.534e-11 ***
## mix          -0.5896970   0.5806401 -1.0156  0.312961    
## pctymle       1.5670818   1.9109317  0.8201  0.414680    
## median_wage   0.0011536   0.0014133  0.8162  0.416848    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>AIC(balanced_model_top_2)</code></pre>
<pre><code>## [1] 28.07462</code></pre>
<p><br />
Three of the five groups have been eliminated, with only the deterrent and demographic groups remaining. We will use step wise regression to evaluate.<br />
</p>
<pre class="r"><code>base_backward = lm(log_crmrte ~ prbarr + prbconv + prbpris 
                             + avgsen + polpc + density
                             + taxpc + pctmin80 + mix + pctymle,
                             data = data_crmrte)</code></pre>
<pre class="r"><code>balanced_model_top_3 &lt;- lm(log_crmrte ~ mix + density
                           + polpc + pctmin80
                           + prbarr + prbconv,
                             data = data_crmrte)
se.balanced_model_top_3 = sqrt(diag(vcovHC(balanced_model_top_3)))
coeftest(balanced_model_top_2, vcov = vcovHC)</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                Estimate  Std. Error t value  Pr(&gt;|t|)    
## (Intercept)  -3.7018614   0.6241512 -5.9310 7.799e-08 ***
## prbarr       -1.9379154   0.4176289 -4.6403 1.381e-05 ***
## prbconv      -0.7650603   0.1360985 -5.6214 2.826e-07 ***
## prbpris      -0.1114352   0.4509889 -0.2471  0.805487    
## avgsen       -0.0046181   0.0137443 -0.3360  0.737770    
## polpc       167.9127417  85.2311288  1.9701  0.052378 .  
## density       0.0977596   0.0343625  2.8450  0.005671 ** 
## taxpc         0.0020705   0.0056750  0.3648  0.716214    
## pctmin80      0.0123893   0.0016202  7.6467 4.534e-11 ***
## mix          -0.5896970   0.5806401 -1.0156  0.312961    
## pctymle       1.5670818   1.9109317  0.8201  0.414680    
## median_wage   0.0011536   0.0014133  0.8162  0.416848    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>AIC(balanced_model_top_3, k=2)</code></pre>
<pre><code>## [1] 21.39003</code></pre>
<p>The difference between the backward and forward model is that the backward model chooses variables for exclusion based on comparing significance while the forward model looks for significance in inclusion. We also used the f-tests (hypothesis tests) to give the backward stepwise regression a head start.<br />
</p>
<p>The backward stepwise regression yielded a more reasonable model so that is the model we are choosing for our balanced model. This model strikes a nice balance between parsimony and explanatory power. The variables included are prbarr, prbconv, polpc, density, pctmin80, and mix. Six out of the original 24 independent variables are included. The adjust r-squared is only 3% lower (77.6% vs. 80.6%). It includes a blend of actionable items for the campaign in the deterrent data as well as demographic variables that perhaps can focus the campaign’s efforts.</p>
<p><strong>3. An Assessment of the CLM Assumptions</strong></p>
<p>We choose our balanced model for the complete assessment of all 6 classical linear model assumptions.</p>
<div class="figure">
<img src="CLM%20Assumptions1.png" alt="MLR 1-4’" />
<p class="caption">MLR 1-4’</p>
</div>
<div id="mlr.1-the-model-is-linear-in-parameters-and-the-error-term" class="section level3">
<h3>MLR.1: The model is linear in parameters ( and the error term)</h3>
<p>we haven’t constrained the error term, so the model can be any joint distribution. Therefore the linear model assumption is not violated</p>
<pre class="r"><code>balanced_model_top_3 &lt;- lm(log_crmrte ~ mix + density
                           + polpc + pctmin80
                           + prbarr + prbconv,
                             data = data_crmrte)</code></pre>
</div>
<div id="mlr.2-random-sampling" class="section level3">
<h3>MLR.2: Random sampling</h3>
<p>First thing to note is that we are dealing with a single cross-section (1987) of a multi-year panel data.</p>
<p>Secondly this is observational data and not experimental so perfect random sampling is hard to achieve.</p>
<p>CORNWELL – TRUMBULL (1994) specifically state they choose panel data because cross – section data were not able to capture the real effect of the crime rate on several independent regressors.</p>
<p>The authors identify that the time-series component of the panel data is able to identify specific characteristics of county heterogeneity, which is correlated with the criminal justice variables.</p>
<p>In exploring the effects of county specific heterogeneity, counties next to each other may exhibit similar behaviour. While that may be valid for prediction model the standard errors may be understated causing violation of random sampling</p>
<p>While the balanced model achieves high level of statistical significance for the co-efficients, it’s important to be mindful of the limitations of the dataset.</p>
<pre class="r"><code>se.balanced_model_top_3 = sqrt(diag(vcovHC(balanced_model_top_3)))
coeftest(balanced_model_top_3, vcov = vcovHC)</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                Estimate  Std. Error  t value  Pr(&gt;|t|)    
## (Intercept)  -3.1966143   0.2371960 -13.4767 &lt; 2.2e-16 ***
## mix          -0.7447751   0.4742158  -1.5705  0.120094    
## density       0.1134850   0.0265199   4.2792 4.997e-05 ***
## polpc       190.5494683  71.9365456   2.6489  0.009666 ** 
## pctmin80      0.0127752   0.0014745   8.6643 3.067e-13 ***
## prbarr       -2.0998396   0.4356245  -4.8203 6.398e-06 ***
## prbconv      -0.8094922   0.1261063  -6.4191 8.051e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="mlr.3-no-perfect-multicollinearity" class="section level3">
<h3>MLR.3: No perfect multicollinearity</h3>
<p>Multicollinearity refers to a situation in which two or more explanatory variables in a multiple regression model are highly linearly related. We have perfect multicollinearity if, for example as in the equation above, the correlation between two independent variables is equal to 100% or negative 100%.</p>
<p>As seen from the correlation matrix below, there is no perfect multicollinearity in the model but we observe some meaningful correlations between (Prbarr, mix) and (Prbarr,density)</p>
<p>These linear realtionships among the X’s don’t invalidate the MLR parameters but they lower precision and increase the std-errors in the mdoel</p>
<pre class="r"><code>balanced_model &lt;- c( &quot;mix&quot;, &quot;density&quot;,&quot;polpc&quot;, &quot;pctmin80&quot;,&quot;prbarr&quot;,&quot;prbconv&quot;)
balanced_model_data &lt;- data_crmrte[balanced_model]
round(cor(balanced_model_data)*100,2) # correlations displayed as % for convenience</code></pre>
<pre><code>##             mix density  polpc pctmin80 prbarr prbconv
## mix      100.00  -13.69   2.41    20.12  41.29  -30.43
## density  -13.69  100.00  15.91    -7.46 -30.27  -22.67
## polpc      2.41   15.91 100.00   -16.91  42.60   17.19
## pctmin80  20.12   -7.46 -16.91   100.00   4.91    6.25
## prbarr    41.29  -30.27  42.60     4.91 100.00   -5.58
## prbconv  -30.43  -22.67  17.19     6.25  -5.58  100.00</code></pre>
</div>
<div id="mlr.4-zero-conditional-mean-exogeneity" class="section level3">
<h3>MLR.4: Zero Conditional Mean / exogeneity</h3>
<p>ZCM is best analysed by studying the regression plots of the residuals. Let’s start by looking at the regression plots of the balanced model</p>
<pre class="r"><code>plot(balanced_model_top_3)</code></pre>
<table><tr><td><img src="lab_3_master_files/figure-html/unnamed-chunk-32-1.png" width=".49\linewidth" style="display: block; margin: auto;" />
</td><td><img src="lab_3_master_files/figure-html/unnamed-chunk-32-2.png" width=".49\linewidth" style="display: block; margin: auto;" /></td></tr>
<tr><td><img src="lab_3_master_files/figure-html/unnamed-chunk-32-3.png" width=".49\linewidth" style="display: block; margin: auto;" /></td><td><img src="lab_3_master_files/figure-html/unnamed-chunk-32-4.png" width=".49\linewidth" style="display: block; margin: auto;" /></td></tr></table>
<p><strong>CLM assumptions analysis from plots</strong></p>
<ul>
<li>Plot 1. The residuals vs. fitted plot indicates that the zero conditional mean assumption is NOT perfectly satisfied but the red line is close enough to zero, a big improvement compared to some of the other models we tested. The non-uniform thickness of the residuals indicates possible heteroskedasticity.</li>
<li>Plot 2. The Q-Q plot shows that the residuals are not perfectly normally distributed, but the log transform of the crime rate improved the positive skew in the data but has introduced some negative skew</li>
<li>Plot 3. The scale location plot indicated the presence of heteroskedasticity especially in the middle where the thickness of the band varies and outliers such as ‘50’ and ‘24’ are generating large standardized residuals</li>
<li>Plot 4. The residuals vs leverage plot shows some of the outliers we had discussed earlier ( 51, 25, 84) but the most significantly outlier is 51 ( having high leverage and Cook’s distance &gt;1). This outlier 51 significantly affects our model estimte and might be worth removing from the data to improve model accuracy.</li>
</ul>
<pre class="r"><code>round(cor(balanced_model_top_3$residuals, balanced_model_data)*100,5)</code></pre>
<pre><code>##      mix density polpc pctmin80 prbarr prbconv
## [1,]   0       0     0        0      0       0</code></pre>
<p>Finally we check the correlation between the X’s and the errors in the model to ensure there is no endogeneity in the model. There is a more extensive discussion of omitted variable and their implication on model endogeniety in section 5 of the report.</p>
<div class="figure">
<img src="CLM%20Assumptions2.png" alt="MLR 5 &amp; 6’" />
<p class="caption">MLR 5 &amp; 6’</p>
</div>
</div>
<div id="mlr.5-homoskedasticity" class="section level3">
<h3>MLR.5: Homoskedasticity</h3>
<p>Homoskedasticity describes a situation in which the error term has the same variance across all values of the independent variables.</p>
<p>The regression plots indicate the presence of some heteroskedasticity in the errors let’s test if they are statistically significant using the Breusch-Pagan test.</p>
<ul>
<li>The Breusch-Pagan test below allows us to test for heteroskedasticity under the</li>
</ul>
<p><span class="math inline">\(H_0: Homeskedasticity\)</span></p>
<pre class="r"><code>bptest(balanced_model_top_3)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  balanced_model_top_3
## BP = 8.6648, df = 6, p-value = 0.1933</code></pre>
<p>From the BP test, surprisingly we find p-value is not statistically significant, therefore we fail to reject <span class="math inline">\(H_0: Homeskedasticity\)</span>.</p>
<p>However, we will still choose to be more conservative and use HC consistent std-errors ( Huber-white Std-errors) using coeftest function from the sandwich package in R. This conservative approach we have taken throughout this report in our model selection process in choosing regressors for different models</p>
</div>
<div id="mlr.6-normality-of-the-error-term" class="section level3">
<h3>MLR.6: Normality of the error term</h3>
<p>Often, if the Y variable is skewed, the error terms will be skewed as well.</p>
<p>We can check the normality using the Q-Q plot to vizualize the distribution of residuals.</p>
<p>We saw in the earlier section that the crime rate has some positive skew, but we were able to reduce the skew by applying log transform to the crime rate.</p>
<p>We can also run a Shapiro - Wilk test for normality of the residuals</p>
<p><span class="math inline">\(H_0: Normality\)</span></p>
<pre class="r"><code>shapiro.test(balanced_model_top_3$residuals)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  balanced_model_top_3$residuals
## W = 0.96118, p-value = 0.008754</code></pre>
<p>The p-value is significant, therefore we reject <span class="math inline">\(H_0: Normality\)</span></p>
<p>The non-normality of the residuals is statisitically significant for this model.</p>
<p>There is some negative skew from outlier 51 in the transformed variable, however, since we have n&gt;30 under CLT we have OLS estimators are normally distributed.</p>
</div>
</div>
<div id="a-regression-table" class="section level1">
<h2>4. A Regression Table</h2>
<p><strong>The results were displayed in stargazer using HC standard errors as part of model selction</strong></p>
<ul>
<li>This section has been fully covered under section 2 of the report</li>
<li>We have include statistical F-tests besides the standard t-tests for regression coefficients to check model validity.</li>
<li>Additionally the practical significance of the model variable chosen have also been discussed in detail</li>
<li>Below is the summary of the regression models and the AIC &amp; BIC scores which provides a parsimony adjusted measure of fit</li>
</ul>
<pre class="r"><code>stargazer(simple_regression_model, all_in_model_log_level, balanced_model_top_1, balanced_model_top_3,
          type = &quot;text&quot;, omit.stat = &quot;f&quot;,
          se = list(se.simple_regression_model, se.all_in_model_log_level, 
                    se.balanced_model_top_1, se.balanced_model_top_3),
          star.cutoffs = c(0.05, 0.01, 0.001))</code></pre>
<pre><code>## 
## ===================================================================================
##                                           Dependent variable:                      
##                     ---------------------------------------------------------------
##                                               log_crmrte                           
##                           (1)             (2)             (3)             (4)      
## -----------------------------------------------------------------------------------
## prbarr                                 -1.889***       -1.963***       -2.100***   
##                                         (0.380)         (0.401)         (0.436)    
##                                                                                    
## prbconv                                -0.656***       -0.767***       -0.809***   
##                                         (0.174)         (0.137)         (0.126)    
##                                                                                    
## prbpris                                 -0.093          -0.076                     
##                                         (0.399)         (0.473)                    
##                                                                                    
## avgsen                                  -0.008          -0.004                     
##                                         (0.016)         (0.014)                    
##                                                                                    
## polpc                                   154.835        176.135*        190.549**   
##                                        (86.523)        (82.588)        (71.937)    
##                                                                                    
## density                0.228***         0.117*          0.114**        0.113***    
##                         (0.030)         (0.054)         (0.035)         (0.027)    
##                                                                                    
## taxpc                                    0.003           0.002                     
##                                         (0.007)         (0.006)                    
##                                                                                    
## west                                    -0.115                                     
##                                         (0.125)                                    
##                                                                                    
## central                                 -0.101                                     
##                                         (0.092)                                    
##                                                                                    
## urban                                   -0.169                                     
##                                         (0.229)                                    
##                                                                                    
## pctmin80                                0.010**        0.013***        0.013***    
##                                         (0.003)         (0.002)         (0.001)    
##                                                                                    
## wcon                                    0.0005                                     
##                                         (0.001)                                    
##                                                                                    
## wtuc                                    0.0001                                     
##                                         (0.001)                                    
##                                                                                    
## wtrd                                    0.0003                                     
##                                         (0.002)                                    
##                                                                                    
## wfir                                    -0.001                                     
##                                         (0.001)                                    
##                                                                                    
## wser                                    -0.0001                                    
##                                         (0.002)                                    
##                                                                                    
## wmfg                                    -0.0002                                    
##                                         (0.001)                                    
##                                                                                    
## wfed                                    0.002*                                     
##                                         (0.001)                                    
##                                                                                    
## wsta                                    -0.001                                     
##                                         (0.001)                                    
##                                                                                    
## wloc                                     0.001                                     
##                                         (0.002)                                    
##                                                                                    
## mix                                     -0.239          -0.730          -0.745     
##                                         (0.626)         (0.540)         (0.474)    
##                                                                                    
## pctymle                                  2.771           1.383                     
##                                         (1.433)         (1.621)                    
##                                                                                    
## Constant               -3.869***       -4.026***       -3.352***       -3.197***   
##                         (0.069)         (0.848)         (0.356)         (0.237)    
##                                                                                    
## -----------------------------------------------------------------------------------
## Observations              90              90              90              90       
## R2                       0.401           0.854           0.796           0.791     
## Adjusted R2              0.394           0.806           0.770           0.776     
## Residual Std. Error 0.427 (df = 88) 0.242 (df = 67) 0.263 (df = 79) 0.260 (df = 83)
## ===================================================================================
## Note:                                                 *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001</code></pre>
<p><strong>Parismony adjusted model performance</strong></p>
<p>Though AIC and BIC are both Maximum Likelihood estimate driven and penalize free parameters in an effort to combat overfitting, they do so in ways that result in significantly different behavior. Lets look at one commonly presented version of the methods (which results from stipulating normally distributed errors and other well behaving assumptions):</p>
<p>AIC = -2<em>ln(likelihood) + 2</em>k, and BIC = -2<em>ln(likelihood) + ln(N)</em>k,</p>
<p>where: k = model degrees of freedom ( K=2 is default for OLS) N = number of observations</p>
<p>The quick explanation is:<br />
</p>
<ul>
<li>AIC is best for prediction as it is asymptotically equivalent to cross-validation.</li>
<li>BIC is best for explanation as it is allows consistent estimation of the underlying data generating process.</li>
</ul>
<p>When N is large the two models will produce quite different results. Then the BIC applies a much larger penalty for complex models, and hence will lead to simpler models than AIC for very large N.</p>
<p>So we check both IC for our model and in both cases a lower value implies a better parsimony adjusted outcome.</p>
<pre class="r"><code>AIC(simple_regression_model)</code></pre>
<pre><code>## [1] 106.2991</code></pre>
<pre class="r"><code>AIC(all_in_model_log_level)</code></pre>
<pre><code>## [1] 21.354</code></pre>
<pre class="r"><code>AIC(balanced_model_top_1)</code></pre>
<pre><code>## [1] 27.4514</code></pre>
<pre class="r"><code>AIC(balanced_model_top_3)</code></pre>
<pre><code>## [1] 21.39003</code></pre>
<pre class="r"><code>BIC(simple_regression_model)</code></pre>
<pre><code>## [1] 113.7985</code></pre>
<pre class="r"><code>BIC(all_in_model_log_level)</code></pre>
<pre><code>## [1] 81.34943</code></pre>
<pre class="r"><code>BIC(balanced_model_top_1)</code></pre>
<pre><code>## [1] 57.44912</code></pre>
<pre class="r"><code>BIC(balanced_model_top_3)</code></pre>
<pre><code>## [1] 41.38851</code></pre>
<br/>
<p><h2>5. Omitted Variables</h2></p>
<p>We’ve identified several key omitted variables that we feel most influence the crime rate but are not represented in the data here.</p>
<ol style="list-style-type: decimal">
<li><p>Unemployment Rate - Unemployment is a key indicator for crime rate. We may be able to infer some indication of the frequency of seasonal or part-time work in the construction or service industries from the <code>wcon</code> or <code>wser</code> variables as they shows an average weekly wage which mght indicate how often workers are employed. However, this estimate is likely not accurate enough to be considered meaningful. The unemployment rate among youth 18-30 would also be meaningful as criminal activity among young adults is higher than that of older adults. Unemployment and inflation rate (see below) may be correlated and may have positive bias on one another.</p></li>
<li><p>Inflation Rate (Consumer Price Index) - Inflation and crime rates are correlated with a positive relationship and the causal link is from inflation and unemployment to crime. <a href="https://www.researchgate.net/publication/236736987_Will_Inflation_Increase_Crime_Rate_New_Evidence_from_Bounds_and_Modified_Wald_Tests">Link</a>. Inﬂation causes the purchasing power to reduce and cost of living to increase, consequently crime rates rise as the inflation rate rises. Because of the lag between price and wage adjustments, inflation lowers the real income of low-skilled labor, but rewards property criminals due to the rising demand and subsequent high profits in the illegal market. Inflation in the year represented, 1987, would not be sufficient though as the reduction in purchasing power does not happen immediately, it takes time for inﬂation to gradually reduce purchasing power. None of the data provided in the study gives us an indication of the inflation rate in a time period before the study. We would expect that this variable would show a positive bias towards crime rate and that it would likely be a large bias. Inflation rate and unemployment may be correlated and may have positive bias on one another.</p></li>
<li><p>Childhood Blood Lead Levels (with 18 year offset) - The lead–crime hypothesis is the proposed link between elevated blood lead levels in children and increased rates of crime, delinquency, and recidivism later in life. Studies linking blood lead levels (BLL) in children to crime rate typically seek to quantify the BLL 17-18 years before the examined crime rate. One such study used a unique dataset linking preschool blood lead levels (BLLs), birth, school, and detention data for 120,000 children born 1990-2004 in Rhode Island, to estimate the impact of lead on behavior <a href="https://www.nber.org/papers/w23392.pdf">Link</a>. We expect that this variable would show a positive bias and that it would likely be a small bias but still significant for any given year as there may be other underlying phenomena driving crime rate in a particular county. There are no variables in the provided data set that would give any insight into this. Denstiy may have a positive correlation with BLL as urban and more industrialized areas typically had greater levels of lead poisoning in groundwater and older housing stock which may contains lead paint.</p></li>
<li><p>Abortion Rates (with 18 year time lag) - Multiple studies have shown a correlation between legalized abortion rates and crime. One study by Donohoe and Leavitt estimated that crime fell roughly 20% between 1997 and 2014 due to legalized abortion. <a href="https://bfi.uchicago.edu/wp-content/uploads/BFI_WP_201975.pdf">Link</a> While it may be difficult to ascertain which counties residents accessing abortion services lived in, we expect that measures of employment and poverty could be correlated to show how a negative bias of abortion rates potentially offset other variables with a positive bias. We estimate that the bias may be small as it could present difficulties in localizing it effectively, but we still believe that it would be significant. There are no variables in the provided data set that would give any insight into this.</p></li>
<li><p>Income Inequality metrics: There are several measures of income inequaity that could be included in the data: <a href="https://www.census.gov/topics/income-poverty/income-inequality/about/metrics/mld.html">Mean Log Deviation</a> or <a href="https://www.census.gov/topics/income-poverty/income-inequality/about/metrics/theil-index.html">Theil Index</a> or <a href="https://www.census.gov/topics/income-poverty/income-inequality/about/metrics/gini-index.html">Gini Index</a> for each of the counties. Income inequality has been shown to have a significant effect on violent crime in particular. One World Bank report states that inequality predicts about half of the variance in murder rates between American states and between countries around the world. <a href="https://siteresources.worldbank.org/DEC/Resources/Crime%26Inequality.pdf">Link</a> Income inequality measures are often measured as 0 (perfectly equal income distribution) to 1 (perfectly unequal income distribution, or 1 household has all the income). We would thus expect these to have a positive bias, in that an increase in income inequality would lead to an increase in violent crime. We expect that the bias would be somewhat smaller as income inequality is correlated specifically with violent crime less than property crime. There are no variables in the provided data set that would give any insight into this.</p></li>
</ol>
<p><h2>6. A Conclusion</h2></p>
<p>Using the 1987 dataset, we were able to identify the key demographic and deterrent variables that affect crime. Our final model shows that arrests (prbarr), convictions (prbconv), and police presence (polpc) are deterrents that affect crime rates. As indicated by the negative coefficients, an increase in arrests and convictions predict a decrease in crime rate, suggesting that these variables are key deterrents in reducing crime. While an increase in police presence predicts an increase in crime rates, it should be noted that this does not indicate that additional police cause an increase in crime. The results suggest that with additional police there will be an increase in reports of crime and apprehension of criminals. In addition, the demographic categories of density, minorities (pctmin80), and offense (mix) are statistically significant variables of crime rate. While the results show a strong statistical relationship with percentage of minorities, this result does not denote a causal effect. This outcome does not indicate that an increase in minorities predicts an increase in crime. Given the complex nature of race relations and criminal justice in the past, the result could also suggest the potential of racial bias within the police force leading to a disproportionate number of minorities being reported for crimes.</p>
<p>While the results of this report have shown that the economic data does not hold statistical significance in our analysis, that is not an indication that economic factors are not key determinants on crime. The wage statistics provided in the dataset, while important, do not address issues of poverty and inequality which are traditional drivers of crime. As described in the report, omitted variables such as unemployment rate, inflation rate, and income inequality are key economic factors that have a significant effect on crime and should be examined further.</p>
<p>Based on the results of our study, we propose a political strategy that will focus on deterrents and demographic factors to address crime. The first recommendation is to increase the police force and focus on increasing police presence in higher density areas. Second, focus resources on increasing arrests and convictions. The increase in police presence will lead to more arrests, which in turn will lead to more convictions. However, it is recommended that punishment for convictions should not be prison sentences unless necessary and justified. Our study suggests that prison sentences are not a significant deterrent of crime, while it also increases overcrowding of the prison system and burdens taxpayers. Lastly, given our first recommendation, increasing the police force too quickly could lead to negative consequences resulting from improper training. We recommend that implicit bias training be a core aspect of the police academy.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
